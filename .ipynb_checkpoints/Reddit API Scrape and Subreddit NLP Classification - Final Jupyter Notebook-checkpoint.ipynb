{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This-or-That Consultants\n",
    "## Executive Summary\n",
    "\n",
    "### Problem Statement:\n",
    "\t\n",
    "\tReddit has suffered an internal attack by a disgruntled ex-employee.  As a parting gift,\n",
    "\tthe ex-employee left the company by replacing all subreddit fields with \" ¯\\_(ツ)_/¯ \".\n",
    "\tAs dedicated Data Analysts, we will build a classification model that can sort certain subreddits\n",
    "\tinto two hats.  Without the subreddit fields being assigned, the links do not work.  \n",
    "\t\tWith a classification model, Reddit will be able to get started on re-assigning the\n",
    "\tfields of each post, at which point the subreddit links will populate and \n",
    "\tbe functional again.\n",
    "\t\tHow successful our model is, of course, will determine whether or not it saves time.\n",
    "\tTo measure the effectiveness of the different models we build, we will measure each model's\n",
    "\taccuracy (correct classifications made divided by all classifications made).\n",
    "\t\tBefore building any models, though, we must: first, retrieve the data; second, clean\n",
    "\tand format the data; and third, vectorize the cleaned text columns (to allow them to be\n",
    "\tincluded as features in the models).  The subreddits we have selected are the Mario Party\n",
    "\tand the Super Smash Ultimate subreddits, two relatively young, active, and present (the\n",
    "\ttwo games were very recently released) subreddits.  We will be using two different methods\n",
    "\tof vectorization: CountVectorize and TF-IDF.\n",
    "\t\tAfter retrieving, cleaning, and vectorizing the data, we will remove certain columns,\n",
    "\tincluding the 'target' column, of course, from a 'features' list.  This list, as implied,\n",
    "\twill contain a list of the columns to be included in the model as a feature and\n",
    "\ttherefore constitute our X's.  Our 'target' column, which contains a binary value of 0 \n",
    "\tor 1 (0 representing that the post came from the Super Smash Ultimate subreddit and 1 \n",
    "\trepresenting that the post came from the Mario Party subreddit), will be used as our y.\n",
    "\t\tNext, we will split the dataset into two: a training set, and a testing set.  By using\n",
    "\tsklearn's train_test_split, we can check the results of our model when run on 'unseen' data,\n",
    "\tgiving ourselves a more realistic reading on the model's effectiveness.\n",
    "\t\tFinally, we will run our data through the modeling process.  We will use four different\n",
    "\tclassification models: 1. Logistic Regression; 2. K Nearest Neighbors; 3. Random Forests;\n",
    "\tand 4. Extra Trees.  After running each model, we will also run the model through a gridsearch,\n",
    "\ttesting different paramaters for each model to further evaluate how well a model could perform.\n",
    "\t\tOnce all models have ran, we will evaluate their accuracy scorings, find the strongest\n",
    "\tperforming model and declare it the model most worth pursuing further.\n",
    "\t\tWhile we continue to improve our model, though, our hope is that reddit would\n",
    "\tpartner with us and use our model to help sort their subreddits.  After all, the reddit team\n",
    "\tknows their product best and would be able to help us most when trying to improve our model.\n",
    "\t\n",
    "\t\n",
    "### Conclusions and Recommendations:\n",
    "\n",
    "\tWhat we found was that our best performing model was Logistic Regression.  We Gridsearched\n",
    "\tit and some of the parameters that resulted were: C = 1.0; penalty = Ridge; tolerance = 0.0001.\n",
    "\t\tWe believe the subreddits may not have had as much overlap as we initially expected; i.e.,\n",
    "\tthe subreddits were more different from each other and therefore were easier to classify.\n",
    "\tWith this being the case, we decided to handicap the model, in a sense, by removing certain\n",
    "\twords from being vectorized (the words found in the subreddits' titles, like Mario, Party, Super,\n",
    "\tSmash, etc.).  We also removed the post authors from the features; we did not want our model to\n",
    "\tbe even partially dependent on this post trait because these two subreddits are relatively young\n",
    "\tand therefore may have concentrated groups of authors, making them a giveaway in this case alone.\n",
    "\tIdeally, our model can work on subreddits that older/have more diverse pools of authors,\n",
    "\tso when building our model, we removed the column entirely from the features.\n",
    "\t\tAfter handicapping the model, though, the results were still strong, with our best model\n",
    "\thaving a Train Set Accuracy Score of .999 and a Test Set Accuracy Score of .950.  We also had a\n",
    "\tstrong performing Extra Trees model, and its number of trees turned out to be 10, implying that\n",
    "\tnot many features were needed to build a strong model (at least with this particular dataset).\n",
    "\t\tOf course, we would love to see how our models work on different pairs of subreddits and\n",
    "\tcompare results.  Does our model work well on the MARIOPARTY/SuperSmashUltimate subreddits,\n",
    "\tor does it also work well on other subreddit pairs?  We'd like to find out.\n",
    "\t\n",
    "\tWe recommend that reddit use our model on the MARIOPARTY/SuperSmashUltimate subreddits!\n",
    "\tThe model seems to work well, at least on this data set, and may work well on others.\n",
    "\t\tAlthough our model seems to be very strong, it cannot guarantee 100% accuracy, therefore\n",
    "\twe have a second suggestion: add a \"Misplaced Post\" button so that users may suggest a post\n",
    "\tas being in the wrong subreddit.  This way, even if a post is misclassified, it can be rectified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function that scrapes reddit's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function 'reddit_to_csv' will take three arguments: 1. the subreddit being scraped; 2. the filename, or the name\n",
    "# the csv file will be given; and 3. the number of requests the user would like to make of reddit's API. \n",
    "\n",
    "def reddit_to_csv(subreddit, filename, n_requests=1):\n",
    "    \n",
    "    #Create an empty list to be used later in function:\n",
    "    posts = []\n",
    "    \n",
    "    #Create User-Agent to avoid 429 res.status_code:\n",
    "    headers = {'User-Agent': 'Knock Knock 914'}\n",
    "    \n",
    "    #Establish that 'after' (a variable used later) is None type:\n",
    "    after = None\n",
    "    \n",
    "    #for loop n_requests iterations (n_requests is established by user):\n",
    "    for i in range(n_requests):\n",
    "        #Print i to inform user how far along function is:\n",
    "        print(i)\n",
    "        \n",
    "        #At first, 'after' will be None, as established above, making params, initially, an empty dictionary\n",
    "        #without any parameters set.  After the first iteration, 'after' will be given a value containing an id\n",
    "        #tag of the last post pulled in that iteration's request, allowing the function to continue looping\n",
    "        #through the next set of posts instead of continuously pulling the same 25 posts, for example.\n",
    "        \n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "            \n",
    "        #Assign 'url' to reddit's base url, plus whatever subreddit the user provides, plus .json for clean results:\n",
    "        url = 'https://www.reddit.com/' + str(subreddit) + '/.json'\n",
    "        \n",
    "        #Set my res variable equal to the results from requests.get, and the parameters set above like 'url' or 'params':\n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        \n",
    "        #Conditional statement to ensure access to the API is approved:\n",
    "        if res.status_code == 200:\n",
    "            \n",
    "            the_json = res.json()\n",
    "            \n",
    "            for x in range(len(the_json['data']['children'])):\n",
    "                \n",
    "                #Create temporary dictionary to add results of each post to:\n",
    "                temp_dict = {}\n",
    "                \n",
    "                #After looking through the json results, I've selected the below information about the posts\n",
    "                #as those that can potentially add value to my model's results.\n",
    "                temp_dict['subreddit'] = the_json['data']['children'][x]['data']['subreddit']\n",
    "                temp_dict['title'] = the_json['data']['children'][x]['data']['title']\n",
    "                temp_dict['post_paragraph'] = the_json['data']['children'][x]['data']['selftext']\n",
    "                temp_dict['clicked'] = the_json['data']['children'][x]['data']['clicked']\n",
    "                temp_dict['ups'] = the_json['data']['children'][x]['data']['ups']\n",
    "                temp_dict['downs'] = the_json['data']['children'][x]['data']['downs']\n",
    "                temp_dict['likes'] = the_json['data']['children'][x]['data']['likes']\n",
    "                temp_dict['category'] = the_json['data']['children'][x]['data']['category']\n",
    "                temp_dict['number_of_comments'] = the_json['data']['children'][x]['data']['num_comments']\n",
    "                temp_dict['score'] = the_json['data']['children'][x]['data']['score']\n",
    "                temp_dict['author_flair_css_class'] = the_json['data']['children'][x]['data']['author_flair_css_class']\n",
    "                temp_dict['subreddit_type'] = the_json['data']['children'][x]['data']['subreddit_type']\n",
    "                \n",
    "                #Add the temporary dictionary to 'posts',the list of each post's dictionary of information:\n",
    "                posts.append(temp_dict)\n",
    "                \n",
    "            after = the_json['data']['after']\n",
    "            \n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "        #Enter a delay of one second in the requests to reddit's API for good internet citizenship:    \n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Turn the list of post dictionaries into a pandas DataFrame:\n",
    "    posts_df = pd.DataFrame(posts)\n",
    "    \n",
    "    #Drop any duplicate rows that may have been pulled:\n",
    "    posts_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    #Rearrange the columns into a more logical order:\n",
    "    posts_df = posts_df[['subreddit', 'title', 'clicked', 'ups', 'downs', 'post_paragraph', 'likes', 'number_of_comments', 'category', 'score', 'author_flair_css_class', 'subreddit_type']]\n",
    "    \n",
    "    #Save the DataFrame as a .csv file:\n",
    "    posts_df.to_csv(str(filename), index = False, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_to_csv(subreddit = 'r/MARIOPARTY',\n",
    "              #n_requests = 150,\n",
    "              #filename = 'mario_party_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_to_csv(subreddit = 'r/SmashBrosUltimate',\n",
    "              #n_requests = 150,\n",
    "              #filename = 'super_smash_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "mario_party_df = pd.read_csv('./mario_party_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>clicked</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>post_paragraph</th>\n",
       "      <th>likes</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>category</th>\n",
       "      <th>score</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Super Mario Party: OUT NOW! Click here to fres...</td>\n",
       "      <td>False</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>wario</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Looking for a group to play Super Mario Party'...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>wario</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Little did I know the weight the last minigame...</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>noflair</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                              title  clicked  \\\n",
       "0  MARIOPARTY  Super Mario Party: OUT NOW! Click here to fres...    False   \n",
       "1  MARIOPARTY  Looking for a group to play Super Mario Party'...    False   \n",
       "2  MARIOPARTY  Little did I know the weight the last minigame...    False   \n",
       "\n",
       "   ups  downs post_paragraph  likes  number_of_comments  category  score  \\\n",
       "0   63      0            NaN    NaN                   4       NaN     63   \n",
       "1   40      0            NaN    NaN                   4       NaN     40   \n",
       "2   21      0            NaN    NaN                   6       NaN     21   \n",
       "\n",
       "  author_flair_css_class subreddit_type  \n",
       "0                  wario         public  \n",
       "1                  wario         public  \n",
       "2                noflair         public  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mario_party_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2052, 12)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mario_party_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_smash_df = pd.read_csv('./super_smash_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>clicked</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>post_paragraph</th>\n",
       "      <th>likes</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>category</th>\n",
       "      <th>score</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SmashBrosUltimate</td>\n",
       "      <td>Super Smash Bros Ultimate Release Day Thread -...</td>\n",
       "      <td>False</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome everyone! It's the day we've all been ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>295</td>\n",
       "      <td>21marth</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SmashBrosUltimate</td>\n",
       "      <td>Mod Apps are now open! Apply if you're interes...</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>36diddykong</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SmashBrosUltimate</td>\n",
       "      <td>Saw the guy in front of me wearing this</td>\n",
       "      <td>False</td>\n",
       "      <td>802</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit                                              title  \\\n",
       "0  SmashBrosUltimate  Super Smash Bros Ultimate Release Day Thread -...   \n",
       "1  SmashBrosUltimate  Mod Apps are now open! Apply if you're interes...   \n",
       "2  SmashBrosUltimate            Saw the guy in front of me wearing this   \n",
       "\n",
       "   clicked  ups  downs                                     post_paragraph  \\\n",
       "0    False  295      0  Welcome everyone! It's the day we've all been ...   \n",
       "1    False   19      0                                                NaN   \n",
       "2    False  802      0                                                NaN   \n",
       "\n",
       "   likes  number_of_comments  category  score author_flair_css_class  \\\n",
       "0    NaN                 366       NaN    295                21marth   \n",
       "1    NaN                  11       NaN     19            36diddykong   \n",
       "2    NaN                  10       NaN    802                    NaN   \n",
       "\n",
       "  subreddit_type  \n",
       "0         public  \n",
       "1         public  \n",
       "2         public  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_smash_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1698, 12)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_smash_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mario_party_df.append(super_smash_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm dataframes were stacked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>clicked</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>post_paragraph</th>\n",
       "      <th>likes</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>category</th>\n",
       "      <th>score</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Super Mario Party: OUT NOW! Click here to fres...</td>\n",
       "      <td>False</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>wario</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Looking for a group to play Super Mario Party'...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>wario</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Little did I know the weight the last minigame...</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>noflair</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                              title  clicked  \\\n",
       "0  MARIOPARTY  Super Mario Party: OUT NOW! Click here to fres...    False   \n",
       "1  MARIOPARTY  Looking for a group to play Super Mario Party'...    False   \n",
       "2  MARIOPARTY  Little did I know the weight the last minigame...    False   \n",
       "\n",
       "   ups  downs post_paragraph  likes  number_of_comments  category  score  \\\n",
       "0   63      0            NaN    NaN                   4       NaN     63   \n",
       "1   40      0            NaN    NaN                   4       NaN     40   \n",
       "2   21      0            NaN    NaN                   6       NaN     21   \n",
       "\n",
       "  author_flair_css_class subreddit_type  \n",
       "0                  wario         public  \n",
       "1                  wario         public  \n",
       "2                noflair         public  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 12)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "2052+1698"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a 'target' column (will equal 1 if the post's subreddit is Mario Party, and 0 if the post's subreddit is Smash Bros. Ultimate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = np.where(df['subreddit'] == 'MARIOPARTY', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for columns that don't have any values and can be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['likes'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I noticed that column 'clicked' is not empty, but the column values are purely False, therefore I will drop 'clicked' as well.  The same for columns 'downs' and 'subreddit_type' which are purely 0's and 'public', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df['clicked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df['downs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['subreddit_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_list = ['likes', 'category', 'clicked', 'downs', 'subreddit_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df_drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set my tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./master_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'title', 'ups', 'post_paragraph', 'number_of_comments',\n",
       "       'score', 'author_flair_css_class', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function that takes a column containing text and returns the lemmatized version in a new 'cleaned' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_cleaner(column, df=df):\n",
    "    #For some reason, I was running into errors trying to run this code until I added the code\n",
    "    #below (df[column+'_clean'] = \"\"), establishing from the beginning that the new column to be created\n",
    "    #exists in the dataframe and contains nothing but empty strings.\n",
    "    \n",
    "    df[column+'_clean'] = \"\"\n",
    "    \n",
    "    #for loop through each row in the column:\n",
    "    for i in range(len(df[column])):\n",
    "        \n",
    "        #Tokenize, or separate, each word in column's string into its own string (prep for lemmatization):\n",
    "        col_tok = []\n",
    "        col_tok.extend(tokenizer.tokenize(df[column][i].lower()))\n",
    "        col_token = []\n",
    "        [col_token.append(s) for s in col_tok if s not in col_token]\n",
    "        \n",
    "        #Lemmatize the words (cut the word to its base/root, for improved model results):\n",
    "        col_lem = []\n",
    "        for x in range(len(col_token)):\n",
    "            col_lem.append(lemmatizer.lemmatize(col_token[x]))\n",
    "        \n",
    "        #Remove characters and numbers (for improved model results, hopefully):\n",
    "        letters_only_col = []\n",
    "        for c in range(len(col_lem)):\n",
    "            letters_only_col.append(re.sub(\"[^a-zA-Z]\", \"\", col_lem[c]))\n",
    "        \n",
    "        #Remove stopwords (for improved model results):\n",
    "        col_words = [w for w in letters_only_col if not w in stopwords.words('english')]\n",
    "        \n",
    "        #Remove 'cheat' words (words that are in the subreddit's name and also in the column)\n",
    "        cheat_words = ['mario', 'party', 'marioparty', 'smash', 'bros', 'ultimate', 'smashbrosultimate', 'super']\n",
    "        col_words = [w for w in letters_only_col if not w in cheat_words]\n",
    "        \n",
    "        #Ensure that there are no 'None' objects in title_words:\n",
    "        col_words = list(filter(None, col_words))\n",
    "\n",
    "        #Join the lemmatized words - stopwords back to one long string (prep for\n",
    "        #vectorization, done outside/after this function):\n",
    "        col_words = \" \".join(col_words)\n",
    "\n",
    "        #Fill new column with 'cleaned' string from column:\n",
    "        df[column+'_clean'][i] = col_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "column_cleaner(column='title', df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  NaN\n",
       "1                                                  NaN\n",
       "2                                                  NaN\n",
       "3                                                  NaN\n",
       "4    First, the objective is the same on all game b...\n",
       "Name: post_paragraph, dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post_paragraph'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post_paragraph'] = df['post_paragraph'].replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "column_cleaner(column='post_paragraph', df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save version of DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df_cleaned.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>post_paragraph</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>target</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>post_paragraph_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Super Mario Party: OUT NOW! Click here to fres...</td>\n",
       "      <td>63</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>wario</td>\n",
       "      <td>1</td>\n",
       "      <td>out now click here to freshen up on what this ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Looking for a group to play Super Mario Party'...</td>\n",
       "      <td>40</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>wario</td>\n",
       "      <td>1</td>\n",
       "      <td>looking for a group to play s online mode with...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Little did I know the weight the last minigame...</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>noflair</td>\n",
       "      <td>1</td>\n",
       "      <td>little did i know the weight last minigame car...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Is...is Rosalina ok?</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>noflair</td>\n",
       "      <td>1</td>\n",
       "      <td>is is rosalina ok</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARIOPARTY</td>\n",
       "      <td>Super Mario Party: Initial Impressions</td>\n",
       "      <td>1</td>\n",
       "      <td>First, the objective is the same on all game b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>noflair</td>\n",
       "      <td>1</td>\n",
       "      <td>initial impression</td>\n",
       "      <td>first the objective is same on all game board ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                              title  ups  \\\n",
       "0  MARIOPARTY  Super Mario Party: OUT NOW! Click here to fres...   63   \n",
       "1  MARIOPARTY  Looking for a group to play Super Mario Party'...   40   \n",
       "2  MARIOPARTY  Little did I know the weight the last minigame...   21   \n",
       "3  MARIOPARTY                               Is...is Rosalina ok?    1   \n",
       "4  MARIOPARTY             Super Mario Party: Initial Impressions    1   \n",
       "\n",
       "                                      post_paragraph  number_of_comments  \\\n",
       "0                                                                      4   \n",
       "1                                                                      4   \n",
       "2                                                                      6   \n",
       "3                                                                      0   \n",
       "4  First, the objective is the same on all game b...                   1   \n",
       "\n",
       "   score author_flair_css_class  target  \\\n",
       "0     63                  wario       1   \n",
       "1     40                  wario       1   \n",
       "2     21                noflair       1   \n",
       "3      1                noflair       1   \n",
       "4      1                noflair       1   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0  out now click here to freshen up on what this ...   \n",
       "1  looking for a group to play s online mode with...   \n",
       "2  little did i know the weight last minigame car...   \n",
       "3                                  is is rosalina ok   \n",
       "4                                 initial impression   \n",
       "\n",
       "                                post_paragraph_clean  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  first the objective is same on all game board ...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CountVectorize Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_column(column, func_df=df):\n",
    "    #Instantiate CountVectorizer:\n",
    "    vect = CountVectorizer()\n",
    "    \n",
    "    #Create temporary variable X_text that takes on the fit/transformed results of the column:\n",
    "    X_text = vect.fit_transform(func_df[column])\n",
    "    \n",
    "    #Turn X_text into an array (prep to easily make a DataFrame):\n",
    "    X_text = X_text.toarray()\n",
    "    \n",
    "    #Create a temporary DataFrame with each word/word-pair/word-group as the columns:\n",
    "    temp_df = pd.DataFrame(X_text,\n",
    "                           columns=vect.get_feature_names())\n",
    "    \n",
    "    #Add the original column name to the beginning of the new columns' names to differentiate from which column\n",
    "    # the vectorized words came from (this may impact the strength of the model):\n",
    "    for i in range(len(temp_df.columns)):\n",
    "        #print(i)\n",
    "        temp_df.rename(columns={temp_df.columns[i]: column + '_' + temp_df.columns[i]}, inplace=True)\n",
    "    \n",
    "    #Combine the two DataFrames:\n",
    "    func_df = pd.concat([func_df, temp_df], axis=1, join_axes=[func_df.index])\n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TF-IDF Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_column(column, func_df=df):\n",
    "    #Instantiate TfidfVectorizer:\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    #Create temporary variable X_text that takes on the fit/transformed results of the column:\n",
    "    X_text = tfidf_vect.fit_transform(func_df[column])\n",
    "    \n",
    "    #Turn X_text into an array (prep to easily make a DataFrame):\n",
    "    X_text = X_text.toarray()\n",
    "    \n",
    "    #Create a temporary DataFrame with each word/word-pair/word-group as the columns:\n",
    "    temp_df = pd.DataFrame(X_text,\n",
    "                           columns=tfidf_vect.get_feature_names())\n",
    "    \n",
    "    #Add the original column name to the beginning of the new columns' names to differentiate from which column\n",
    "    # the tf-idf vectorized words came from (this may impact the strength of the model):\n",
    "    for i in range(len(temp_df.columns)):\n",
    "        #print(i)\n",
    "        temp_df.rename(columns={temp_df.columns[i]: column + '_' + temp_df.columns[i]}, inplace=True)\n",
    "    \n",
    "    #Combine the two DataFrames:\n",
    "    func_df = pd.concat([func_df, temp_df], axis=1, join_axes=[func_df.index])\n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = count_vec_column(func_df=df, column='title_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['post_paragraph'] = df['post_paragraph'].replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = count_vec_column(func_df=df, column='post_paragraph_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9304"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df_cleaned_vected.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the 'author_flair_css_class' from the features list (I will 'get_dummies' on this column later on to see if it affects the strength of my models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'title', 'ups', 'post_paragraph', 'number_of_comments',\n",
       "       'score', 'author_flair_css_class', 'target', 'title_clean',\n",
       "       'post_paragraph_clean',\n",
       "       ...\n",
       "       'post_paragraph_clean_zelda', 'post_paragraph_clean_zero',\n",
       "       'post_paragraph_clean_zio', 'post_paragraph_clean_ziodyne',\n",
       "       'post_paragraph_clean_zionga', 'post_paragraph_clean_zombie',\n",
       "       'post_paragraph_clean_zone', 'post_paragraph_clean_zoom',\n",
       "       'post_paragraph_clean_zoomed', 'post_paragraph_clean_zr'],\n",
       "      dtype='object', length=9304)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit',\n",
       " 'title',\n",
       " 'ups',\n",
       " 'post_paragraph',\n",
       " 'number_of_comments',\n",
       " 'score',\n",
       " 'author_flair_css_class',\n",
       " 'target',\n",
       " 'title_clean',\n",
       " 'post_paragraph_clean']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = ['subreddit', 'target', 'title', 'title_clean', 'post_paragraph', 'post_paragraph_clean', 'author_flair_css_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [i for i in features if i not in del_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"CountVectorized\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8993661817456399"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9960881934566145"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94136460554371"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.5, 1.0, 25],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(logreg, param_grid=my_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.5, 1.0, 25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985775248933144"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9498933901918977"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"CountVectorized\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6642978126595259"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424608819345661"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685501066098081"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'n_neighbors': [5, 10, 25],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(knn, param_grid=my_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [5, 10, 25], 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899786780383795"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests:\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"CountVectorized\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8862017098529309"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975106685633002"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381663113006397"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [18, 20, 25],\n",
    "    'max_depth': [4, 10, 20],\n",
    "    'max_features': ['auto', 1.0, 2, 3]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(rf, param_grid = my_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'n_estimators': [18, 20, 25], 'max_depth': [4, 10, 20], 'max_features': ['auto', 1.0, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096728307254623"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.826226012793177"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"CountVectorized\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8897595419760475"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9477611940298507"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [18, 20, 25],\n",
    "    'max_depth': [4, 10, 20],\n",
    "    'max_features': ['auto', 1.0, 2, 3]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(et, param_grid = my_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'n_estimators': [18, 20, 25], 'max_depth': [4, 10, 20], 'max_features': ['auto', 1.0, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903271692745377"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835820895522388"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['subreddit', 'title', 'ups', 'post_paragraph', 'number_of_comments',\n",
    "       'score', 'author_flair_css_class', 'target', 'title_clean',\n",
    "       'post_paragraph_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df = tfidf_column(func_df=df, column='title_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['post_paragraph'] = df['post_paragraph'].replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = tfidf_column(func_df=df, column='post_paragraph_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9304"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df_cleaned_tfidf.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the 'author_flair_css_class' from the features list (I will 'get_dummies' on this column later on to see if it affects the strength of my models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'title', 'ups', 'post_paragraph', 'number_of_comments',\n",
       "       'score', 'author_flair_css_class', 'target', 'title_clean',\n",
       "       'post_paragraph_clean',\n",
       "       ...\n",
       "       'zelda.1', 'zero.1', 'zio', 'ziodyne', 'zionga', 'zombie', 'zone',\n",
       "       'zoom', 'zoomed', 'zr'],\n",
       "      dtype='object', length=9304)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit',\n",
       " 'title',\n",
       " 'ups',\n",
       " 'post_paragraph',\n",
       " 'number_of_comments',\n",
       " 'score',\n",
       " 'author_flair_css_class',\n",
       " 'target',\n",
       " 'title_clean',\n",
       " 'post_paragraph_clean']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = ['subreddit', 'target', 'title', 'title_clean', 'post_paragraph', 'post_paragraph_clean', 'author_flair_css_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [i for i in features if i not in del_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"TF-IDF'ed\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8950923079373676"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822190611664295"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9189765458422174"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.04268735e-03,  6.06753110e-05, -3.04268735e-03, ...,\n",
       "        -1.96434998e-01,  0.00000000e+00,  9.75458709e-02]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.5, 1.0, 25],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(logreg, param_grid=my_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.5, 1.0, 25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985775248933144"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9445628997867804"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"TF-IDF'ed\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6582505220505189"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8040540540540541"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6673773987206824"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'n_neighbors': [5, 10, 25],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(knn, param_grid=my_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [5, 10, 25], 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6823027718550106"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests:\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"TF-IDF'ed\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8758869928448928"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971550497866287"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275053304904051"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [18, 20, 25],\n",
    "    'max_depth': [4, 10, 20],\n",
    "    'max_features': ['auto', 1.0, 2, 3]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(rf, param_grid = my_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'n_estimators': [18, 20, 25], 'max_depth': [4, 10, 20], 'max_features': ['auto', 1.0, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9267425320056899"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8272921108742004"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees\n",
    "\n",
    "#### To note:\n",
    "\n",
    "* Authors of the posts are not included in the Features/Inputs\n",
    "* The 'title' and 'post_paragraph' columns were cleaned and \"TF-IDF'ed\"\n",
    "* Words contained in the subreddit title names (like Mario, Party, Super, etc.) have been removed to ensure that \n",
    "the model is not depending on these for accuracy; we do not want to assume that future titles will always be so descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8947384589478283"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9307036247334755"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [18, 20, 25],\n",
    "    'max_depth': [4, 10, 20],\n",
    "    'max_features': ['auto', 1.0, 2, 3]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(et, param_grid = my_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'n_estimators': [18, 20, 25], 'max_depth': [4, 10, 20], 'max_features': ['auto', 1.0, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9274537695590327"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835820895522388"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
